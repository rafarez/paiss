{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratical session on Image Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical session, we will explore how to perform typical tasks associated with image retrieval. Students will be able to download this IPython/Jupyter notebook after the class in order to perform the experiments also at home. \n",
    "\n",
    "**Link to the slides**: [PDF 30MB](https://www.dropbox.com/s/mjmh8al5wg6731j/18_07_PAISS_practical_session.pdf?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "This code requires Python 3, Pytorch 0.4, and Jupyter Notebook. Follow the instructions below to install all the necessary dependencies.\n",
    "\n",
    "### Installing dependencies\n",
    "\n",
    "First, download and install the appropriate version of miniconda following the instructions for [MacOS](https://conda.io/docs/user-guide/install/macos.html) or [Linux](https://conda.io/docs/user-guide/install/linux.html).\n",
    "\n",
    "Then run the following commands:\n",
    "\n",
    "```\n",
    "source $HOME/miniconda3/bin/activate #Activates your conda environment\n",
    "conda install numpy matplotlib ipython scikit-learn jupyter\n",
    "conda install pytorch torchvision faiss-cpu -c pytorch\n",
    "```\n",
    "\n",
    "\n",
    "### Downloading the code, dataset, and models\n",
    "\n",
    "First, clone this repository:\n",
    "\n",
    "```\n",
    "cd $HOME/my_projects\n",
    "git clone https://github.com/rafarez/paiss.git\n",
    "```\n",
    "\n",
    "Then, you will need to download 4 files:\n",
    "\n",
    "- oxbuild_images.tgz (1.8GB)\n",
    "- gt\\_files\\_170407.tgz (280KB)\n",
    "- features.tgz (579MB)\n",
    "- models.tgz (328MB)\n",
    "\n",
    "and store them in the appropriate paths.\n",
    "\n",
    "_Note:_ All paths in this section are relative to the root directory of this repository.\n",
    "\n",
    "#### Oxford dataset\n",
    "\n",
    "On Linux/MacOS, execute the following:\n",
    "\n",
    "```\n",
    "cd $HOME/my_projects/paiss\n",
    "wget www.robots.ox.ac.uk/~vgg/data/oxbuildings/oxbuild_images.tgz -O images.tgz\n",
    "mkdir -p data/oxford5k/jpg && tar -xzf images.tgz -C data/oxford5k/jpg\n",
    "wget www.robots.ox.ac.uk/~vgg/data/oxbuildings/gt_files_170407.tgz -O gt_files.tgz\n",
    "mkdir -p data/oxford5k/lab && tar -xzf gt_files.tgz -C data/oxford5k/lab\n",
    "```\n",
    "\n",
    "\n",
    "#### Features and models\n",
    "\n",
    "On Linux/MacOS, execute the following:\n",
    "\n",
    "```\n",
    "cd $HOME/my_projects/paiss\n",
    "wget https://www.dropbox.com/s/gr404xlfr4021pw/features.tgz?dl=1 -O features.tgz\n",
    "tar -xzf features.tgz -C data\n",
    "wget https://www.dropbox.com/s/mr4risqu7t9neel/models.tgz?dl=1 -O models.tgz\n",
    "tar -xzf models.tgz -C data\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Running the notebook\n",
    "\n",
    "```\n",
    "cd $HOME/my_projects/paiss\n",
    "jupyter notebook --ip='localhost' --port=61230 --NotebookApp.token=''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatives\n",
    "\n",
    "We start by importing the necessary modules and fixing a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import pdb\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "from datasets import create\n",
    "from archs import *\n",
    "from utils.test import extract_query\n",
    "from utils.tsne import do_tsne\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate the Oxford dataset, that we will use in all following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Oxford 5k database\n",
    "dataset = create('Oxford')\n",
    "\n",
    "# get the label vector\n",
    "labels = dataset.get_label_vector()\n",
    "classes = dataset.get_label_names()\n",
    "\n",
    "# load the dictionary of the available models and features\n",
    "with open('data/models.json', 'r') as fp:\n",
    "    models_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Creating a network with the AlexNet architecture\n",
    "\n",
    "As a first step, we will be creating a neural network implementing the AlexNet architecture to use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantate the model for the first experiment\n",
    "model_1a = alexnet_imagenet()\n",
    "\n",
    "# show the network details\n",
    "print(model_1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfeats = np.load(models_dict['alexnet-cls-imagenet-fc7']['dataset'])\n",
    "\n",
    "# Q: What does each line of the matrix feats represent? Where does the dimension of these lines comes from and how do we extract these features?\n",
    "# Hint: uncomment and run the following command\n",
    "# model_1a_test = alexnet_imagenet_fc7(); print(model_1a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(dfeats[:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize top results for a given query\n",
    "dataset.vis_top(dfeats, q_idx, ap_flag=True, out_image_file=out_image)\n",
    "\n",
    "if args.show_tsne:\n",
    "    # run t-SNE\n",
    "    do_tsne(dfeats, labels, classes, sec='1a')\n",
    "    # Q: What can be observe from the t-SNE visualization? Which classes 'cluster' well? Which do not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Finetuning the created network on the Landmarks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1b = alexnet_lm()\n",
    "print(model_1b)\n",
    "input(\"Check session.py. Press Enter to continue...\")\n",
    "# Q: Why do we change the last layer of the AlexNet architecture? How do we initialize the layers of model_1b for finetuning?\n",
    "\n",
    "dfeats = np.load(models_dict['alexnet-cls-lm-fc7']['dataset'])\n",
    "qfeats = np.load(models_dict['alexnet-cls-lm-fc7']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, ap_flag=True, out_image_file=out_image)\n",
    "\n",
    "if args.show_tsne:\n",
    "    # run t-SNE\n",
    "    do_tsne(dfeats, labels, classes, sec='1b')\n",
    "    # Q: How does the visualization change after finetuning? What about the top results?\n",
    "\n",
    "# question on how the architecture demands the resize of the input images (specifically, the fully connected layers) ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Replacing last max pooling layer with GeM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1c = alexnet_GeM()\n",
    "print(model_1c)\n",
    "input(\"Check session.py. Press Enter to continue...\")\n",
    "# Q: For this model, we remove all fully connected layers (classifier layers) and replace the last max pooling layer by an aggregation pooling layer (more details about this layer in the next subsection)\n",
    "\n",
    "dfeats = np.load(models_dict['alexnet-cls-lm-gem']['dataset'])\n",
    "qfeats = np.load(models_dict['alexnet-cls-lm-gem']['queries'])\n",
    "print(dfeats.shape)\n",
    "input(\"Check session.py. Press Enter to continue...\")\n",
    "# Q: Why does the size of the feature representation changes? Why does the size of the feature representation is important for a image retrieval task?\n",
    "dataset.vis_top(dfeats, q_idx, ap_flag=True, out_image_file=out_image)\n",
    "\n",
    "if args.show_tsne:\n",
    "    do_tsne(dfeats, labels, classes, sec='1c')\n",
    "    # Q: How does the aggregation layer changes the t-SNE visualization? Can we see some structure in the clusters of similarly labeled images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) ResNet18 architecture with GeM pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = resnet18()\n",
    "model_1d = resnet18_GeM()\n",
    "print(model_0.adpool)\n",
    "print(model_1d.adpool)\n",
    "input(\"Check session.py. Press Enter to continue...\")\n",
    "# Q: Why do we change the average pooling layer of the original Resnet18\n",
    "# architecture for a generalized mean pooling? What operation is the layer\n",
    "# model_1d.adpool doing?\n",
    "# Hint: You can see the code of the generalized mean pooling in file pooling.py\n",
    "\n",
    "# load oxford features from ResNet18 model\n",
    "dfeats = np.load(models_dict['resnet18-cls-lm-gem']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-cls-lm-gem']['queries'])\n",
    "print(norm(dfeats[:10], axis=1))\n",
    "print(dfeats.shape)\n",
    "# visualize top results for a given query index\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True, out_image_file=out_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1d')\n",
    "# Q: How does this model compare with model 1c, that was trained in the same dataset for the same task? How does is compare to the finetuned models of 1b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) PCA Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a PCA learnt on landmarks to whiten the output features of 'resnet18-cls-lm-gem'\n",
    "dfeats = np.load(models_dict['resnet18-cls-lm-gem-pcaw']['dataset'])\n",
    "qfeats = np.load(models_dict['resnet18-cls-lm-gem-pcaw']['queries'])\n",
    "dataset.vis_top(dfeats, q_idx, q_feat=qfeats[q_idx], ap_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the data with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_tsne(dfeats, labels, classes, sec='1e-1')\n",
    "\n",
    "# run t-SNE including unlabeled images\n",
    "do_tsne(dfeats, labels, classes, sec='1e-2', show_unlabeled=True)\n",
    "# Q: What can we say about the separation of data when included unlabeled images? And the distribution of the unlabeled features? How can we train a model to separate labeled from unlabeled data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
